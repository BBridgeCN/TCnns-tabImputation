{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 7\n",
    "num_epochs = 50\n",
    "dropout_ratio = 0.5\n",
    "\n",
    "data_path = 'datas/dd2/FoodConsumption.csv'\n",
    "mechanism = 'mnar'\n",
    "method = 'random'\n",
    "\n",
    "test_size = 0.3\n",
    "use_cuda = True\n",
    "batch_size  = 1 # not in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = pd.read_csv(data_path, on_bad_lines='skip')\n",
    "print(datas[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test \n",
    "def per_MissVal(df):\n",
    "    # Calculer le nombre total de valeurs manquantes\n",
    "    total_mv = df.isnull().sum().sum()\n",
    "    # Calculer le nombre total de valeurs dans le DataFrame\n",
    "    total_val = df.size\n",
    "    # Calculer le pourcentage de valeurs manquantes\n",
    "    perc = (total_mv / total_val) * 100\n",
    "    return perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"data missing values\", per_MissVal(datas), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Detect column types ----------------\n",
    "numeric_features = datas.select_dtypes(include=[\"int64\", \"float64\"]).columns.to_list()\n",
    "categorical_features = datas.select_dtypes(include=[\"object\", \"category\"]).columns.to_list()\n",
    "\n",
    "# ---------------- Define numeric imputer using HistGradientBoostingRegressor ----------------\n",
    "# Custom imputer using HistGradientBoostingRegressor for numeric data\n",
    "class HGBRImputer:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.features = None\n",
    "\n",
    "    # Add y=None to match sklearn's fit signature\n",
    "    def fit(self, X, y=None):\n",
    "        self.features = X.columns\n",
    "        for col in self.features:\n",
    "            is_missing = X[col].isnull()\n",
    "            if is_missing.sum() == 0:\n",
    "                continue\n",
    "            train_idx = ~is_missing\n",
    "            # Fill missing values in predictors with median for training\n",
    "            X_train = X.loc[train_idx].drop(columns=col).fillna(X.loc[train_idx].median())\n",
    "            y_train = X.loc[train_idx, col]\n",
    "            model = HistGradientBoostingRegressor(\n",
    "                learning_rate = 0.1, \n",
    "                max_depth = 15, \n",
    "                min_samples_leaf = 30, \n",
    "                l2_regularization = 0.1, \n",
    "                max_bins = 255, \n",
    "                scoring = 'neg_mean_squared_error',\n",
    "                max_iter=100,\n",
    "                max_leaf_nodes=31,\n",
    "                random_state=42,\n",
    "                early_stopping=True,\n",
    "                validation_fraction=0.25,\n",
    "                n_iter_no_change=10\n",
    "            )\n",
    "            model.fit(X_train, y_train)\n",
    "            self.models[col] = model\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_filled = X.copy()\n",
    "        for col, model in self.models.items():\n",
    "            missing_idx = X_filled[col].isnull()\n",
    "            if missing_idx.sum() == 0:\n",
    "                continue\n",
    "            # Fill missing predictor features with median before prediction\n",
    "            X_pred = X_filled.loc[missing_idx].drop(columns=col).fillna(X_filled.median())\n",
    "            preds = model.predict(X_pred)\n",
    "            X_filled.loc[missing_idx, col] = preds\n",
    "        return X_filled\n",
    "\n",
    "\n",
    "# ---------------- Define transformers ----------------\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", HGBRImputer()),\n",
    "    (\"scaler\", MinMaxScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # handle missing categorical\n",
    "    (\"ordinal\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))\n",
    "])\n",
    "\n",
    "# ---------------- Column Transformer ----------------\n",
    "transformers = []\n",
    "if len(numeric_features) > 0:\n",
    "    transformers.append((\"num\", numeric_transformer, numeric_features))\n",
    "if len(categorical_features) > 0:\n",
    "    transformers.append((\"cat\", categorical_transformer, categorical_features))\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=transformers,\n",
    "    sparse_threshold=0\n",
    ")\n",
    "\n",
    "# ---------------- Full Pipeline ----------------\n",
    "pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor)])\n",
    "\n",
    "# ---------------- Apply on dataset ----------------\n",
    "# pipeline.fit(datas)\n",
    "processed = pipeline.fit_transform(datas)\n",
    "\n",
    "# Convert back to DataFrame with column names\n",
    "all_features = []\n",
    "if len(numeric_features) > 0:\n",
    "    all_features.extend(numeric_features)\n",
    "if len(categorical_features) > 0:\n",
    "    all_features.extend(categorical_features)\n",
    "\n",
    "df_processed = pd.DataFrame(processed, columns=all_features)\n",
    "\n",
    "print(df_processed.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_processed.values #datas.values\n",
    "\n",
    "if use_cuda and not torch.cuda.is_available():\n",
    "    print(\"---\"* 20)\n",
    "    print(\"CUDA is not available. Switching to CPU.\")\n",
    "    use_cuda = False\n",
    "\n",
    "\n",
    "rows, cols = data.shape\n",
    "shuffled_index = np.random.permutation(rows)\n",
    "train_index = shuffled_index[:int(rows*(1-test_size))]\n",
    "test_index = shuffled_index[int(rows*(1-test_size)):]\n",
    "\n",
    "\n",
    "train_data = data[train_index, :]\n",
    "test_data = data[test_index, :]\n",
    "\n",
    "print(\"\")\n",
    "print(train_data[:5])\n",
    "print(\"\")\n",
    "print(test_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "#df = pd.read_csv('data/dd3/titanic.csv')\n",
    "df0 = pd.DataFrame(train_data)  \n",
    "df1 = pd.DataFrame(test_data)\n",
    "#df = df.drop(columns=['Cabin'])\n",
    "print(\"train_data missing values\", per_MissVal(df0), \"%\", \"--\", \"test_data missing values\", per_MissVal(df1), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of rows and columns\", data.shape)\n",
    "print()\n",
    "print(\"size of the dataset\", data.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_method(raw_data, mechanism='mcar', method='uniform') :\n",
    "    \n",
    "    data = raw_data.copy()\n",
    "    rows, cols = data.shape\n",
    "    \n",
    "    # missingness threshold\n",
    "    t = 0.1\n",
    "    \n",
    "    if mechanism == 'mcar' :\n",
    "    \n",
    "        if method == 'uniform' :\n",
    "            # uniform random vector\n",
    "            v = np.random.uniform(size=(rows, cols))\n",
    "\n",
    "            # missing values where v<=t\n",
    "            mask = (v<=t)\n",
    "            data[mask] = 0\n",
    "\n",
    "        elif method == 'random' :\n",
    "            # only half of the attributes to have missing value\n",
    "            missing_cols = np.random.choice(cols, cols//2)\n",
    "            c = np.zeros(cols, dtype=bool)\n",
    "            c[missing_cols] = True\n",
    "\n",
    "            # uniform random vector\n",
    "            v = np.random.uniform(size=(rows, cols))\n",
    "\n",
    "            # missing values where v<=t\n",
    "            mask = (v<=t)*c\n",
    "            data[mask] = 0\n",
    "\n",
    "        else :\n",
    "            print(\"Error : There are no such method\")\n",
    "            raise\n",
    "    \n",
    "    elif mechanism == 'mnar' :\n",
    "        \n",
    "        if method == 'uniform' :\n",
    "            # randomly sample two attributes\n",
    "            sample_cols = np.random.choice(cols, 2)\n",
    "\n",
    "            # calculate then median m1, m2\n",
    "            m1, m2 = np.median(data[:,sample_cols], axis=0)\n",
    "            # uniform random vector\n",
    "            v = np.random.uniform(size=(rows, cols))\n",
    "\n",
    "            # missing values where (v<=t) and (x1 <= m1 or x2 >= m2)\n",
    "            m1 = data[:,sample_cols[0]] <= m1\n",
    "            m2 = data[:,sample_cols[1]] >= m2\n",
    "            m = (m1*m2)[:, np.newaxis]\n",
    "\n",
    "            mask = m*(v<=t)\n",
    "            data[mask] = 0\n",
    "\n",
    "\n",
    "        elif method == 'random' :\n",
    "            # only half of the attributes to have missing value\n",
    "            missing_cols = np.random.choice(cols, cols//2)\n",
    "            c = np.zeros(cols, dtype=bool)\n",
    "            c[missing_cols] = True\n",
    "\n",
    "            # randomly sample two attributes\n",
    "            sample_cols = np.random.choice(cols, 2)\n",
    "\n",
    "            # calculate ther median m1, m2\n",
    "            m1, m2 = np.median(data[:,sample_cols], axis=0)\n",
    "            # uniform random vector\n",
    "            v = np.random.uniform(size=(rows, cols))\n",
    "\n",
    "            # missing values where (v<=t) and (x1 <= m1 or x2 >= m2)\n",
    "            m1 = data[:,sample_cols[0]] <= m1\n",
    "            m2 = data[:,sample_cols[1]] >= m2\n",
    "            m = (m1*m2)[:, np.newaxis]\n",
    "\n",
    "            mask = m*(v<=t)*c\n",
    "            data[mask] = 0\n",
    "\n",
    "        else :\n",
    "            print(\"Error : There is no such method\")\n",
    "            raise\n",
    "    \n",
    "    else :\n",
    "        print(\"Error : There is no such mechanism\")\n",
    "        raise\n",
    "        \n",
    "    return data, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_data, mask = missing_method(test_data, mechanism=mechanism, method=method)\n",
    "\n",
    "missed_data = torch.from_numpy(missed_data).float()\n",
    "train_data = torch.from_numpy(train_data).float()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# VÃ©rifier si le device est bien CUDA ou CPU\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"CUDA Device Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA Device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"CUDA not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        self.drop_out = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(dim+theta*0, dim+theta*1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim+theta*1, dim+theta*2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim+theta*2, dim+theta*3)\n",
    "        )\n",
    "            \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(dim+theta*3, dim+theta*2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim+theta*2, dim+theta*1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim+theta*1, dim+theta*0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.dim)\n",
    "        x_missed = self.drop_out(x)\n",
    "        \n",
    "        z = self.encoder(x_missed)\n",
    "        out = self.decoder(z)\n",
    "        \n",
    "        out = out.view(-1, self.dim)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(dim=cols).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), momentum=0.99, lr=0.01, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cost_list = []\n",
    "early_stop = False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    total_batch = len(train_data) // batch_size\n",
    "    \n",
    "    for i, batch_data in enumerate(train_loader):\n",
    "        \n",
    "        batch_data = batch_data.to(device)\n",
    "        \n",
    "        reconst_data = model(batch_data)\n",
    "        cost = loss(reconst_data, batch_data)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        if (i+1) % (total_batch//2) == 0:\n",
    "            print('Epoch [%d/%d], lter [%d/%d], Loss: %.6f'\n",
    "                 %(epoch+1, num_epochs, i+1, total_batch, cost.item()))\n",
    "            \n",
    "        # early stopping rule 1 : MSE < 1e-06\n",
    "        if cost.item() < 1e-06 :\n",
    "            early_stop = True\n",
    "            break\n",
    "            \n",
    "#         early stopping rule 2 : simple moving average of length 5\n",
    "#         sometimes it doesn't work well.\n",
    "#         if len(cost_list) > 5 :\n",
    "#            if cost.item() > np.mean(cost_list[-5:]):\n",
    "#                early_stop = True\n",
    "#                break\n",
    "                \n",
    "        cost_list.append(cost.item())\n",
    "\n",
    "    if early_stop :\n",
    "        break\n",
    "        \n",
    "print(\"Learning Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "#filled_data = model(missed_data.to(device))\n",
    "#filled_data = filled_data.cpu().detach().numpy()\n",
    "\n",
    "with torch.no_grad():   # safer: avoid gradients\n",
    "    filled_data = model(missed_data.to(device))\n",
    "    filled_data = filled_data.cpu().detach().numpy()\n",
    "\n",
    "rmse_sum = 0\n",
    "num_valid_columns = 0\n",
    "\n",
    "for i in range(cols) :\n",
    "    if mask[:,i].sum() > 0 :\n",
    "        y_actual = test_data[:,i][mask[:,i]]\n",
    "        y_predicted = filled_data[:,i][mask[:,i]]\n",
    "\n",
    "        rmse = sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "        rmse_sum += rmse\n",
    "        num_valid_columns += 1\n",
    "    \n",
    "#print(\"RMSE_SUM :\", rmse_sum)\n",
    "print(f'RMSE_SUM: {rmse_sum:.3f}')\n",
    "average_rmse = rmse_sum / num_valid_columns\n",
    "print(\"-----------\")\n",
    "print(f'Average RMSE: {average_rmse:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "#from numpy import sqrt\n",
    "\n",
    "mse_sum = 0\n",
    "num_valid_columns = 0  # To count columns with valid data\n",
    "\n",
    "for i in range(cols):\n",
    "    if mask[:, i].sum() > 0:  # Check if there are any missing values\n",
    "        y_actual = test_data[:, i][mask[:, i]]\n",
    "        y_predicted = filled_data[:, i][mask[:, i]]\n",
    "\n",
    "        # Calculate RMSE for the current column\n",
    "        mse = mean_squared_error(y_actual, y_predicted)\n",
    "        mse_sum += mse\n",
    "        num_valid_columns += 1  # Increment valid column count\n",
    "\n",
    "# Calculate average RMSE if there are valid columns\n",
    "if num_valid_columns > 0:\n",
    "    #print(\"MSE sum:\", mse_sum)\n",
    "    print(f'MSE (sum) Score: {mse_sum:.3f}')\n",
    "    print(\"-------\")\n",
    "    average_mse = mse_sum / num_valid_columns\n",
    "    #print(\"Average MSE:\", average_mse)\n",
    "    print(f'Average MSE: {average_mse:.3f}')\n",
    "else:\n",
    "    print(\"No valid columns to calculate MSE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae_sum = 0\n",
    "num_valid_columns = 0\n",
    "\n",
    "for i in range(cols):\n",
    "    if mask[:, i].sum() > 0:\n",
    "        y_actual = test_data[:, i][mask[:, i]]\n",
    "        y_predicted = filled_data[:, i][mask[:, i]]\n",
    "\n",
    "        mae = mean_absolute_error(y_actual, y_predicted)\n",
    "        mae_sum += mae\n",
    "        num_valid_columns += 1\n",
    "\n",
    "print(f'MAE (sum) Score: {mae_sum:.3f}')\n",
    "print(\"------------\")\n",
    "average_mae = mae_sum / num_valid_columns \n",
    "print(f'Average MAE Score: {average_mae:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_sum = 0\n",
    "count = 0\n",
    "\n",
    "for i in range(cols):\n",
    "    if mask[:, i].sum() > 0:  # only evaluate where ground truth exists\n",
    "        y_actual = test_data[:, i][mask[:, i]]\n",
    "        y_predicted = filled_data[:, i][mask[:, i]]\n",
    "        \n",
    "        # Convert continuous values to discrete labels by rounding or thresholding\n",
    "        # Here we round values to nearest integer as an example\n",
    "        y_actual_discrete = np.rint(y_actual).astype(int)\n",
    "        y_predicted_discrete = np.rint(y_predicted).astype(int)\n",
    "\n",
    "        # Now compute accuracy_score on discrete labels\n",
    "        acc = accuracy_score(y_actual_discrete, y_predicted_discrete)\n",
    "        acc_sum += acc\n",
    "        count += 1\n",
    "\n",
    "if count > 0:\n",
    "    avg_acc = acc_sum / count\n",
    "    print(f'Accuracy Score (sum): {acc_sum:.3f}')\n",
    "    print(f'Average Accuracy Score: {avg_acc:.3f}')\n",
    "else:\n",
    "    print(\"No valid columns for Accuracy calculation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The time used to execute this is given below\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
