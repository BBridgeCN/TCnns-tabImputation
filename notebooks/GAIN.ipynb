{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creator: Dhanajit Brahma\n",
    "\n",
    "Adapted from the original implementation in tensorflow from here: https://github.com/jsyoon0823/GAIN\n",
    "\n",
    "Generative Adversarial Imputation Networks (GAIN) Implementation on Letter and Spam Dataset\n",
    "\n",
    "Reference: J. Yoon, J. Jordon, M. van der Schaar, \"GAIN: Missing Data Imputation using Generative Adversarial Nets,\" ICML, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Packages\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = 'datas/dd2/BostonHousing.csv'  # 'Letter.csv' for Letter dataset an 'Spam.csv' for Spam dataset\n",
    "use_gpu = False  # set it to True to use GPU and False to use CPU\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = pd.read_csv(dataset_file, on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Detect column types ----------------\n",
    "numeric_features = datas.select_dtypes(include=[\"int64\", \"float64\"]).columns.to_list()\n",
    "categorical_features = datas.select_dtypes(include=[\"object\", \"category\"]).columns.to_list()\n",
    "\n",
    "# ---------------- Define numeric imputer using HistGradientBoostingRegressor ----------------\n",
    "# Custom imputer using HistGradientBoostingRegressor for numeric data\n",
    "class HGBRImputer:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.features = None\n",
    "\n",
    "    # Add y=None to match sklearn's fit signature\n",
    "    def fit(self, X, y=None):\n",
    "        self.features = X.columns\n",
    "        for col in self.features:\n",
    "            is_missing = X[col].isnull()\n",
    "            if is_missing.sum() == 0:\n",
    "                continue\n",
    "            train_idx = ~is_missing\n",
    "            # Fill missing values in predictors with median for training\n",
    "            X_train = X.loc[train_idx].drop(columns=col).fillna(X.loc[train_idx].median())\n",
    "            y_train = X.loc[train_idx, col]\n",
    "            model = HistGradientBoostingRegressor(\n",
    "                learning_rate = 0.1, \n",
    "                max_depth = 15, \n",
    "                min_samples_leaf = 30, \n",
    "                l2_regularization = 0.1, \n",
    "                max_bins = 255, \n",
    "                scoring = 'neg_mean_squared_error',\n",
    "                max_iter=100,\n",
    "                max_leaf_nodes=31,\n",
    "                random_state=42,\n",
    "                early_stopping=True,\n",
    "                validation_fraction=0.25,\n",
    "                n_iter_no_change=10\n",
    "            )\n",
    "            model.fit(X_train, y_train)\n",
    "            self.models[col] = model\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_filled = X.copy()\n",
    "        for col, model in self.models.items():\n",
    "            missing_idx = X_filled[col].isnull()\n",
    "            if missing_idx.sum() == 0:\n",
    "                continue\n",
    "            # Fill missing predictor features with median before prediction\n",
    "            X_pred = X_filled.loc[missing_idx].drop(columns=col).fillna(X_filled.median())\n",
    "            preds = model.predict(X_pred)\n",
    "            X_filled.loc[missing_idx, col] = preds\n",
    "        return X_filled\n",
    "\n",
    "\n",
    "# ---------------- Define transformers ----------------\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", HGBRImputer()),\n",
    "    (\"scaler\", MinMaxScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # handle missing categorical\n",
    "    (\"ordinal\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))\n",
    "])\n",
    "\n",
    "# ---------------- Column Transformer ----------------\n",
    "transformers = []\n",
    "if len(numeric_features) > 0:\n",
    "    transformers.append((\"num\", numeric_transformer, numeric_features))\n",
    "if len(categorical_features) > 0:\n",
    "    transformers.append((\"cat\", categorical_transformer, categorical_features))\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=transformers,\n",
    "    sparse_threshold=0\n",
    ")\n",
    "\n",
    "# ---------------- Full Pipeline ----------------\n",
    "pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor)])\n",
    "\n",
    "# ---------------- Apply on dataset ----------------\n",
    "# pipeline.fit(datas)\n",
    "processed = pipeline.fit_transform(datas)\n",
    "\n",
    "# Convert back to DataFrame with column names\n",
    "all_features = []\n",
    "if len(numeric_features) > 0:\n",
    "    all_features.extend(numeric_features)\n",
    "if len(categorical_features) > 0:\n",
    "    all_features.extend(categorical_features)\n",
    "\n",
    "df_processed = pd.DataFrame(processed, columns=all_features)\n",
    "\n",
    "print(df_processed.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0= df_processed.to_csv('datas/BostonHousing_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% System Parameters\n",
    "# 1. Mini batch size\n",
    "mb_size = 128\n",
    "# 2. Missing rate\n",
    "p_miss = 0.2\n",
    "# 3. Hint rate\n",
    "p_hint = 0.9\n",
    "# 4. Loss Hyperparameters\n",
    "alpha = 10\n",
    "# 5. Train Rate\n",
    "train_rate = 0.8\n",
    "\n",
    "#%% Data\n",
    "\n",
    "# Data generation\n",
    "datas_file = 'datas/BostonHousing_processed.csv'  \n",
    "Data = pd.read_csv(datas_file, on_bad_lines='skip')\n",
    "#Data = d.values #np.loadtxt(datas_file, delimiter=\",\")#, skiprows=1)\n",
    "\n",
    "# Parameters\n",
    "No = len(Data)\n",
    "Dim = len(Data.iloc[0,:])\n",
    "\n",
    "# Hidden state dimensions\n",
    "H_Dim1 = Dim\n",
    "H_Dim2 = Dim\n",
    "\n",
    "# Normalization (0 to 1)\n",
    "Min_Val = np.zeros(Dim)\n",
    "Max_Val = np.zeros(Dim)\n",
    "\n",
    "for i in range(Dim):\n",
    "    Min_Val = np.min(Data.iloc[:,i])\n",
    "    Data.iloc[:,i] = Data.iloc[:,i] - Min_Val\n",
    "    Max_Val = np.max(Data.iloc[:,i])\n",
    "    Data.iloc[:,i] = Data.iloc[:,i] / (Max_Val + 1e-6) \n",
    "\n",
    "\n",
    "#%% Missing introducing\n",
    "p_miss_vec = p_miss * np.ones((Dim,1)) \n",
    "   \n",
    "Missing = np.zeros((No,Dim))\n",
    "\n",
    "for i in range(Dim):\n",
    "    A = np.random.uniform(0., 1., size = [len(Data),])\n",
    "    B = A > p_miss_vec[i]\n",
    "    Missing[:,i] = 1.*B\n",
    "\n",
    "    \n",
    "#%% Train Test Division    \n",
    "   \n",
    "idx = np.random.permutation(No)\n",
    "\n",
    "Train_No = int(No * train_rate)\n",
    "Test_No = No - Train_No\n",
    "    \n",
    "# Train / Test Features\n",
    "trainX = Data.iloc[idx[:Train_No],:]\n",
    "testX = Data.iloc[idx[Train_No:],:]\n",
    "\n",
    "# Train / Test Missing Indicators\n",
    "trainM = Missing[idx[:Train_No],:]\n",
    "testM = Missing[idx[Train_No:],:]\n",
    "\n",
    "#%% Necessary Functions\n",
    "\n",
    "# 1. Xavier Initialization Definition\n",
    "# def xavier_init(size):\n",
    "#     in_dim = size[0]\n",
    "#     xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "#     return tf.random_normal(shape = size, stddev = xavier_stddev)\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return np.random.normal(size = size, scale = xavier_stddev)\n",
    "    \n",
    "# Hint Vector Generation\n",
    "def sample_M(m, n, p):\n",
    "    A = np.random.uniform(0., 1., size = [m, n])\n",
    "    B = A > p\n",
    "    C = 1.*B\n",
    "    return C\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAIN Architecture   \n",
    "GAIN Consists of 3 Components\n",
    "- Generator\n",
    "- Discriminator\n",
    "- Hint Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 1. Discriminator\n",
    "if use_gpu is True:\n",
    "    D_W1 = torch.tensor(xavier_init([Dim*2, H_Dim1]),requires_grad=True, device=\"cuda\")     # Data + Hint as inputs\n",
    "    D_b1 = torch.tensor(np.zeros(shape = [H_Dim1]),requires_grad=True, device=\"cuda\")\n",
    "\n",
    "    D_W2 = torch.tensor(xavier_init([H_Dim1, H_Dim2]),requires_grad=True, device=\"cuda\")\n",
    "    D_b2 = torch.tensor(np.zeros(shape = [H_Dim2]),requires_grad=True, device=\"cuda\")\n",
    "\n",
    "    D_W3 = torch.tensor(xavier_init([H_Dim2, Dim]),requires_grad=True, device=\"cuda\")\n",
    "    D_b3 = torch.tensor(np.zeros(shape = [Dim]),requires_grad=True, device=\"cuda\")       # Output is multi-variate\n",
    "else:\n",
    "    D_W1 = torch.tensor(xavier_init([Dim*2, H_Dim1]),requires_grad=True)     # Data + Hint as inputs\n",
    "    D_b1 = torch.tensor(np.zeros(shape = [H_Dim1]),requires_grad=True)\n",
    "\n",
    "    D_W2 = torch.tensor(xavier_init([H_Dim1, H_Dim2]),requires_grad=True)\n",
    "    D_b2 = torch.tensor(np.zeros(shape = [H_Dim2]),requires_grad=True)\n",
    "\n",
    "    D_W3 = torch.tensor(xavier_init([H_Dim2, Dim]),requires_grad=True)\n",
    "    D_b3 = torch.tensor(np.zeros(shape = [Dim]),requires_grad=True)       # Output is multi-variate\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n",
    "\n",
    "#%% 2. Generator\n",
    "if use_gpu is True:\n",
    "    G_W1 = torch.tensor(xavier_init([Dim*2, H_Dim1]),requires_grad=True, device=\"cuda\")     # Data + Mask as inputs (Random Noises are in Missing Components)\n",
    "    G_b1 = torch.tensor(np.zeros(shape = [H_Dim1]),requires_grad=True, device=\"cuda\")\n",
    "\n",
    "    G_W2 = torch.tensor(xavier_init([H_Dim1, H_Dim2]),requires_grad=True, device=\"cuda\")\n",
    "    G_b2 = torch.tensor(np.zeros(shape = [H_Dim2]),requires_grad=True, device=\"cuda\")\n",
    "\n",
    "    G_W3 = torch.tensor(xavier_init([H_Dim2, Dim]),requires_grad=True, device=\"cuda\")\n",
    "    G_b3 = torch.tensor(np.zeros(shape = [Dim]),requires_grad=True, device=\"cuda\")\n",
    "else:\n",
    "    G_W1 = torch.tensor(xavier_init([Dim*2, H_Dim1]),requires_grad=True)     # Data + Mask as inputs (Random Noises are in Missing Components)\n",
    "    G_b1 = torch.tensor(np.zeros(shape = [H_Dim1]),requires_grad=True)\n",
    "\n",
    "    G_W2 = torch.tensor(xavier_init([H_Dim1, H_Dim2]),requires_grad=True)\n",
    "    G_b2 = torch.tensor(np.zeros(shape = [H_Dim2]),requires_grad=True)\n",
    "\n",
    "    G_W3 = torch.tensor(xavier_init([H_Dim2, Dim]),requires_grad=True)\n",
    "    G_b3 = torch.tensor(np.zeros(shape = [Dim]),requires_grad=True)\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAIN Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 1. Generator\n",
    "def generator(new_x,m):\n",
    "    inputs = torch.cat(dim = 1, tensors = [new_x,m])  # Mask + Data Concatenate\n",
    "    G_h1 = F.relu(torch.matmul(inputs, G_W1) + G_b1)\n",
    "    G_h2 = F.relu(torch.matmul(G_h1, G_W2) + G_b2)   \n",
    "    G_prob = torch.sigmoid(torch.matmul(G_h2, G_W3) + G_b3) # [0,1] normalized Output\n",
    "    \n",
    "    return G_prob\n",
    "\n",
    "#%% 2. Discriminator\n",
    "def discriminator(new_x, h):\n",
    "    inputs = torch.cat(dim = 1, tensors = [new_x,h])  # Hint + Data Concatenate\n",
    "    D_h1 = F.relu(torch.matmul(inputs, D_W1) + D_b1)  \n",
    "    D_h2 = F.relu(torch.matmul(D_h1, D_W2) + D_b2)\n",
    "    D_logit = torch.matmul(D_h2, D_W3) + D_b3\n",
    "    D_prob = torch.sigmoid(D_logit)  # [0,1] Probability Output\n",
    "    \n",
    "    return D_prob\n",
    "\n",
    "#%% 3. Other functions\n",
    "# Random sample generator for Z\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(0., 0.01, size = [m, n])        \n",
    "\n",
    "# Mini-batch generation\n",
    "def sample_idx(m, n):\n",
    "    A = np.random.permutation(m)\n",
    "    idx = A[:n]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAIN Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(M, New_X, H):\n",
    "    # Generator\n",
    "    G_sample = generator(New_X,M)\n",
    "    # Combine with original data\n",
    "    Hat_New_X = New_X * M + G_sample * (1-M)\n",
    "\n",
    "    # Discriminator\n",
    "    D_prob = discriminator(Hat_New_X, H)\n",
    "\n",
    "    #%% Loss\n",
    "    D_loss = -torch.mean(M * torch.log(D_prob + 1e-8) + (1-M) * torch.log(1. - D_prob + 1e-8))\n",
    "    return D_loss\n",
    "\n",
    "def generator_loss(X, M, New_X, H):\n",
    "    #%% Structure\n",
    "    # Generator\n",
    "    G_sample = generator(New_X,M)\n",
    "\n",
    "    # Combine with original data\n",
    "    Hat_New_X = New_X * M + G_sample * (1-M)\n",
    "\n",
    "    # Discriminator\n",
    "    D_prob = discriminator(Hat_New_X, H)\n",
    "\n",
    "    #%% Loss\n",
    "    G_loss1 = -torch.mean((1-M) * torch.log(D_prob + 1e-8))\n",
    "    MSE_train_loss = torch.mean((M * New_X - M * G_sample)**2) / torch.mean(M)\n",
    "\n",
    "    G_loss = G_loss1 + alpha * MSE_train_loss \n",
    "\n",
    "    #%% MSE Performance metric\n",
    "    MSE_test_loss = torch.mean(((1-M) * X - (1-M)*G_sample)**2) / torch.mean(1-M)\n",
    "    return G_loss, MSE_train_loss, MSE_test_loss\n",
    "    \n",
    "def test_loss(X, M, New_X):\n",
    "    #%% Structure\n",
    "    # Generator\n",
    "    G_sample = generator(New_X,M)\n",
    "\n",
    "    #%% MSE Performance metric\n",
    "    MSE_test_loss = torch.mean(((1-M) * X - (1-M)*G_sample)**2) / torch.mean(1-M)\n",
    "    return MSE_test_loss, G_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_D = torch.optim.Adam(params=theta_D)\n",
    "optimizer_G = torch.optim.Adam(params=theta_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%% Start Iterations\n",
    "for it in tqdm(range(5000)):    \n",
    "    \n",
    "    #%% Inputs\n",
    "    mb_idx = sample_idx(Train_No, mb_size)\n",
    "    X_mb = trainX.iloc[mb_idx,:]  \n",
    "    \n",
    "    Z_mb = sample_Z(mb_size, Dim) \n",
    "    M_mb = trainM[mb_idx,:]  \n",
    "    H_mb1 = sample_M(mb_size, Dim, 1-p_hint)\n",
    "    H_mb = M_mb * H_mb1\n",
    "    \n",
    "    New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n",
    "    X_mb = torch.tensor(X_mb.to_numpy(), device='cuda' if use_gpu else 'cpu')\n",
    "    #M_mb = torch.tensor(M_mb.to_numpy(), device='cuda' if use_gpu else 'cpu')\n",
    "    New_X_mb = torch.tensor(New_X_mb.to_numpy(), device='cuda' if use_gpu else 'cpu')\n",
    "\n",
    "    \n",
    "    if use_gpu is True:\n",
    "        X_mb = torch.tensor(X_mb, device=\"cuda\")\n",
    "        M_mb = torch.tensor(M_mb, device=\"cuda\")\n",
    "        H_mb = torch.tensor(H_mb, device=\"cuda\")\n",
    "        New_X_mb = torch.tensor(New_X_mb, device=\"cuda\")\n",
    "    else:\n",
    "        X_mb = torch.tensor(X_mb)\n",
    "        M_mb = torch.tensor(M_mb)\n",
    "        H_mb = torch.tensor(H_mb)\n",
    "        New_X_mb = torch.tensor(New_X_mb)\n",
    "    \n",
    "    optimizer_D.zero_grad()\n",
    "    D_loss_curr = discriminator_loss(M=M_mb, New_X=New_X_mb, H=H_mb)\n",
    "    D_loss_curr.backward()\n",
    "    optimizer_D.step()\n",
    "    \n",
    "    optimizer_G.zero_grad()\n",
    "    G_loss_curr, MSE_train_loss_curr, MSE_test_loss_curr = generator_loss(X=X_mb, M=M_mb, New_X=New_X_mb, H=H_mb)\n",
    "    G_loss_curr.backward()\n",
    "    optimizer_G.step()    \n",
    "        \n",
    "    #%% Intermediate Losses\n",
    "    if it % 100 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Train_loss: {:.4}'.format(np.sqrt(MSE_train_loss_curr.item())))\n",
    "        print('Test_loss: {:.4}'.format(np.sqrt(MSE_test_loss_curr.item())))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_mb = sample_Z(Test_No, Dim) \n",
    "M_mb = testM\n",
    "X_mb = testX\n",
    "        \n",
    "New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n",
    "X_mb = torch.tensor(X_mb.to_numpy(), device='cuda' if use_gpu else 'cpu')\n",
    "New_X_mb = torch.tensor(New_X_mb.to_numpy(), device='cuda' if use_gpu else 'cpu')\n",
    "\n",
    "\n",
    "if use_gpu is True:\n",
    "    X_mb = torch.tensor(X_mb, device='cuda')\n",
    "    M_mb = torch.tensor(M_mb, device='cuda')\n",
    "    New_X_mb = torch.tensor(New_X_mb, device='cuda')\n",
    "else:\n",
    "    X_mb = torch.clone().detach()#.tensor(X_mb)\n",
    "    M_mb = torch.tensor(M_mb)\n",
    "    New_X_mb = torch.clone().detach()#.tensor(New_X_mb)\n",
    "    \n",
    "MSE_final, Sample = test_loss(X=X_mb, M=M_mb, New_X=New_X_mb)\n",
    "        \n",
    "print('Final Test RMSE: ' + str(np.sqrt(MSE_final.item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X_mb, M_mb, and New_X_mb are torch tensors with true, mask, and imputed data\n",
    "\n",
    "# Convert tensors to numpy arrays for sklearn compatibility, focusing only on missing positions\n",
    "with torch.no_grad():\n",
    "    imputed_values = New_X_mb[M_mb == 0].cpu().numpy()\n",
    "    true_values = X_mb[M_mb == 0].cpu().numpy()\n",
    "\n",
    "# Calculate R2 score\n",
    "if len(true_values) > 1:  # Ensure more than 1 sample for R2\n",
    "    r2 = r2_score(true_values, imputed_values)\n",
    "else:\n",
    "    r2 = float('nan')  # Not defined for fewer than 2 samples\n",
    "\n",
    "# Existing RMSE and MAE calculations...\n",
    "\n",
    "MSE_final, Sample = test_loss(X=X_mb, M=M_mb, New_X=New_X_mb)\n",
    "rmse = np.sqrt(MSE_final.item())\n",
    "mae = torch.mean(torch.abs(imputed_values - true_values)).item() \n",
    "\n",
    "print(f'Final Test RMSE: {rmse:.6f}')\n",
    "print(f'Final Test MAE: {mae:.6f}')\n",
    "print(f'Final Test R2 Score: {r2:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume the test_loss function returns MSE and Sample count (as in your context)\n",
    "\n",
    "MSE_final, Sample = test_loss(X=X_mb, M=M_mb, New_X=New_X_mb)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(MSE_final.item())\n",
    "\n",
    "# Calculate MAE manually over missing entries (mask == 0)\n",
    "with torch.no_grad():\n",
    "    # Imputed values at missing positions\n",
    "    imputed_values = New_X_mb[M_mb == 0]\n",
    "    # True values at missing positions\n",
    "    true_values = X_mb[M_mb == 0]\n",
    "    # Mean Absolute Error\n",
    "    mae = torch.mean(torch.abs(imputed_values - true_values)).item()\n",
    "\n",
    "# Calculate Accuracy:\n",
    "# For continuous values, Accuracy is not a standard metric.\n",
    "# However, if data is categorical or discretized, accuracy can be defined as:\n",
    "# Accuracy = (Number of correct imputations) / (Total missing entries)\n",
    "# Here is a simple example assuming categorical values.\n",
    "\n",
    "with torch.no_grad():\n",
    "    if X_mb.dtype == torch.float32:  # Continuous data likely\n",
    "        accuracy = None  # Accuracy generally not defined\n",
    "    else:\n",
    "        # For categorical case: count exact matches\n",
    "        correct = (imputed_values == true_values).sum().item()\n",
    "        total_missing = imputed_values.numel()\n",
    "        accuracy = correct / total_missing if total_missing > 0 else None\n",
    "\n",
    "print(f'Final Test RMSE: {rmse:.6f}')\n",
    "print(f'Final Test MSE: {MSE_final.item():.6f}')\n",
    "print(f'Final Test MAE: {mae:.6f}')\n",
    "if accuracy is not None:\n",
    "    print(f'Final Test Accuracy: {accuracy:.6f}')\n",
    "else:\n",
    "    print(\"Accuracy metric not applicable for continuous data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = M_mb * X_mb + (1-M_mb) * Sample\n",
    "print(\"Imputed test data:\")\n",
    "# np.set_printoptions(formatter={'float': lambda x: \"{0:0.8f}\".format(x)})\n",
    "\n",
    "if use_gpu is True:\n",
    "    print(imputed_data.cpu().detach().numpy())\n",
    "else:\n",
    "    print(imputed_data.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
