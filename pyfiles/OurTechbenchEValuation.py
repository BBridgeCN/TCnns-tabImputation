# -*- coding: utf-8 -*-
"""CopyOurTech-4benchEVa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vyQNRsXvSIPix-BBpsgp-Nv0f4jvUFzS
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import numpy as np
import pandas as pd
from scipy.stats import wilcoxon, friedmanchisquare, rankdata
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import spearmanr

# ============================================================================
# FINAL CORRECTED RMSE DATA
# ============================================================================
datasets = ['Boston', 'Glass', 'Sonar', 'Breast(Wdbc)', 'Credit']
our_technique = np.array([0.214, 0.146, 0.187, 0.131, 0.325])
mida = np.array([0.232, 0.221, 0.174, 0.170, 0.353])
gain = np.array([0.201, 0.189, 0.216, 0.233, 0.319])
dae = np.array([0.520, 0.240, 0.650, 0.142, 0.487])
mice = np.array([0.690, 0.300, 0.223, 0.132, 0.530])

methods_dict = {
    'Our Technique': our_technique,
    'MIDA': mida,
    'GAIN': gain,
    'DAE': dae,
    'MICE': mice
}

print("="*80)

# ============================================================================
# 1. FRIEDMAN TEST
# ============================================================================
print("\n1. FRIEDMAN TEST (Overall Comparison)")
print("-" * 80)

all_data = np.vstack([our_technique, mida, gain, dae, mice])
friedman_stat, friedman_p = friedmanchisquare(*[methods_dict[m] for m in methods_dict.keys()])

print(f"χ²({len(methods_dict)-1}) = {friedman_stat:.4f}")
print(f"p-value = {friedman_p:.6f}")
if friedman_p < 0.001:
    print("Result: HIGHLY SIGNIFICANT (p < 0.001) ✓")
elif friedman_p < 0.05:
    print("Result: SIGNIFICANT (p < 0.05) ✓")

import numpy as np
from scipy.stats import friedmanchisquare

# ============================================================================
# 1. FRIEDMAN TEST (Overall Comparison)
# ============================================================================

print("\n1. FRIEDMAN TEST (Overall Comparison)")
print("-" * 80)

# Combine method results into a single array. Assuming each is shape (num_datasets,)
all_data = np.vstack([our_technique, mida, gain, dae, mice])

# Extract method names and arrays in consistent order
methods = list(methods_dict.keys())
method_arrays = [methods_dict[m] for m in methods]

# Run Friedman test
friedman_stat, friedman_p = friedmanchisquare(*method_arrays)

# Print test statistic and p-value with formatting
print(f"Chi-squared({len(methods) - 1}) = {friedman_stat:.4f}")
print(f"p-value = {friedman_p:.6f}")

# Interpret result with clear thresholds
if friedman_p < 0.001:
    significance = "HIGHLY SIGNIFICANT (p < 0.001) ✓"
elif friedman_p < 0.05:
    significance = "SIGNIFICANT (p < 0.05) ✓"
else:
    significance = "NOT SIGNIFICANT"

print(f"Result: {significance}")

# ============================================================================
# 2. MEAN RANKS
# ============================================================================
print("\n2. MEAN RANKS ANALYSIS")
print("-" * 80)

ranks = np.zeros_like(all_data)
for i in range(all_data.shape[1]):
    ranks[:, i] = rankdata(all_data[:, i])

mean_ranks = ranks.mean(axis=1)
rank_df = pd.DataFrame({
    'Method': list(methods_dict.keys()),
    'Mean Rank': mean_ranks,
    'Position': rankdata(mean_ranks).astype(int)
}).sort_values('Mean Rank')

print(rank_df.to_string(index=False))
print(f"\n✓ Best: {rank_df.iloc[0]['Method']} (rank={rank_df.iloc[0]['Mean Rank']:.2f})")

# ============================================================================
# 3. PAIRWISE WILCOXON TESTS WITH EFFECT SIZES
# ============================================================================
print("\n3. PAIRWISE WILCOXON TESTS")
print("-" * 80)

def rank_biserial_correlation(x, y):
    n = len(x)
    differences = x - y
    if np.all(differences == 0):
        return 0.0
    ranks = rankdata(np.abs(differences))
    ranks_positive = np.sum(ranks[differences > 0])
    ranks_negative = np.sum(ranks[differences < 0])
    r = (ranks_positive - ranks_negative) / (n * (n + 1) / 2)
    return r

def cohens_d(x, y):
    pooled_std = np.sqrt((np.var(x, ddof=1) + np.var(y, ddof=1)) / 2)
    if pooled_std == 0:
        return 0.0
    return (np.mean(x) - np.mean(y)) / pooled_std

comparison_results = []
for name, values in list(methods_dict.items())[1:]:
    differences = our_technique - values
    wins = np.sum(our_technique < values)
    losses = np.sum(our_technique > values)

    try:
        stat, p_value = wilcoxon(our_technique, values, alternative='less')
    except:
        stat = 0
        p_value = 1.0

    r_bc = rank_biserial_correlation(our_technique, values)
    cohens = cohens_d(our_technique, values)
    mean_diff = np.mean(values - our_technique)
    pct_improvement = (mean_diff / np.mean(values)) * 100 if np.mean(values) != 0 else 0

    comparison_results.append({
        'Baseline': name,
        'W': stat,
        'p-value': p_value,
        'r': r_bc,
        'd': cohens,
        'Wins': wins,
        'Losses': losses,
        'Δ RMSE': mean_diff,
        '% Δ': pct_improvement
    })

comp_df = pd.DataFrame(comparison_results)
print(comp_df.to_string(index=False))

# Bonferroni correction
alpha = 0.05
bonferroni_alpha = alpha / len(comparison_results)
print(f"\nBonferroni-corrected α = {bonferroni_alpha:.4f}")

print("\nSignificance Assessment:")
for _, row in comp_df.iterrows():
    if row['p-value'] < 0.001:
        sig = "*** (p<0.001, survives Bonferroni)"
    elif row['p-value'] < 0.01:
        sig = "** (p<0.01, survives Bonferroni)"
    elif row['p-value'] < bonferroni_alpha:
        sig = "* (survives Bonferroni)"
    elif row['p-value'] < 0.05:
        sig = "† (p<0.05, uncorrected only)"
    else:
        sig = "n.s."
    print(f"  vs {row['Baseline']:8s}: p={row['p-value']:.4f} {sig:35s} | r={row['r']:+.2f}, W/L={row['Wins']}/{row['Losses']}")

# Publication style
plt.style.use('seaborn-v0_8-paper')
plt.rcParams.update({
    'font.family': 'serif',
    'legend.fontsize': 14,
    'legend.title_fontsize': 13,
    'xtick.labelsize': 15,
    'ytick.labelsize': 15,
    'legend.framealpha': 0.9,
    'font.size': 16,
    'axes.labelsize': 15,
    'axes.titlesize': 16,
    'figure.dpi': 100,
    'savefig.dpi': 300
})

# FINAL DATA
datasets = ['Boston', 'Glass', 'Sonar', 'Breast', 'Credit']
our_technique = np.array([0.214, 0.146, 0.187, 0.131, 0.325])
mida = np.array([0.232, 0.221, 0.174, 0.17 , 0.353])
gain = np.array([0.201, 0.189, 0.216, 0.233, 0.319])
dae = np.array([0.520, 0.240, 0.650, 0.142, 0.487])
mice = np.array([0.690, 0.300, 0.223, 0.132, 0.530])

methods_dict = {
    'Our Technique': our_technique,
    'MIDA': mida,
    'GAIN': gain,
    'DAE': dae,
    'MICE': mice
}

# Calculate ranks
all_data = np.vstack([our_technique, mida, gain, dae, mice])
ranks = np.zeros_like(all_data)
for i in range(all_data.shape[1]):
    ranks[:, i] = rankdata(all_data[:, i])
mean_ranks = ranks.mean(axis=1)
rank_df = pd.DataFrame({
    'Method': list(methods_dict.keys()),
    'Mean Rank': mean_ranks
}).sort_values('Mean Rank')

# Effect sizes
def rank_biserial(x, y):
    n = len(x)
    diff = x - y
    if np.all(diff == 0): return 0.0
    ranks = rankdata(np.abs(diff))
    r_pos = np.sum(ranks[diff > 0])
    r_neg = np.sum(ranks[diff < 0])
    return (r_pos - r_neg) / (n * (n + 1) / 2)

effects = [rank_biserial(our_technique, methods_dict[m])
           for m in ['MIDA', 'GAIN', 'DAE', 'MICE']]

# Colors
colors = {
    'Our Technique': '#2ecc71',
    'GAIN': '#3498db',
    'MIDA': '#9b59b6',
    'DAE': '#e74c3c',
    'MICE': '#f39c12'
}

# Create a folder to save figures in Google Drive
import os
output_folder = '/content/drive/MyDrive/MIDA-our/Results/figures'
os.makedirs(output_folder, exist_ok=True)

# ============================================================================
# PANEL A: RMSE Trajectories
# ============================================================================
fig_a, ax1 = plt.subplots(figsize=(11, 9))
for method in methods_dict.keys():
    vals = methods_dict[method]
    marker = 'o' if method == 'Our Technique' else 's'
    lw = 2.5 if method == 'Our Technique' else 1.8
    ax1.plot(datasets, vals, marker=marker, label=method,
             linewidth=lw, color=colors[method], markersize=8 if method=='Our Technique' else 6)

for i, dataset in enumerate(datasets):
    min_rmse = min([methods_dict[m][i] for m in methods_dict.keys()])
    best = [m for m in methods_dict.keys() if methods_dict[m][i] == min_rmse][0]
    if best in ['Our Technique', 'GAIN', 'MIDA']:
        ax1.scatter(i, min_rmse, s=200, facecolors='none',
                   edgecolors='gold', linewidths=2.5, zorder=10)

ax1.set_xlabel('Dataset', fontweight='bold')
ax1.set_ylabel('RMSE', fontweight='bold')
ax1.set_title('RMSE performance across datasets', fontweight='bold')
ax1.legend(loc='upper right', framealpha=0.9)
ax1.grid(True, alpha=0.3, linestyle='--')
ax1.tick_params(axis='x', rotation=30, labelsize=14)
ax1.tick_params(axis='y', labelsize=14)
plt.savefig(os.path.join(output_folder, "figure1_panelA_RMSE_Trajectories.pdf"), bbox_inches='tight')
plt.savefig(os.path.join(output_folder, "figure1_panelA_RMSE_Trajectories.png"), dpi=300, bbox_inches='tight')
plt.close(fig_a)

# ============================================================================
# PANEL B: Mean Ranks
# ============================================================================
fig_b, ax2 = plt.subplots(figsize=(11, 9))
methods_order = rank_df['Method'].tolist()
colors_rank = [colors[m] for m in methods_order]
bars = ax2.barh(range(len(methods_order)), rank_df['Mean Rank'],
                color=colors_rank, alpha=0.8, edgecolor='black', linewidth=1.2)

for i, (idx, row) in enumerate(rank_df.iterrows()):
    ax2.text(row['Mean Rank'] + 0.1, i, f"{row['Mean Rank']:.2f}",
             va='center', fontweight='bold')
    ax2.text(1.35, i, f'#{i+1}', va='center', ha='right',
             fontweight='bold', color='gray')

ax2.set_yticks(range(len(methods_order)))
ax2.set_yticklabels(methods_order)
ax2.tick_params(axis='x', labelsize=14)
ax2.tick_params(axis='y', labelsize=14)
ax2.set_xlabel('Mean rank', fontweight='bold')
ax2.set_title('Overall performance ranking', fontweight='bold') #, loc='left')
ax2.invert_yaxis()
ax2.grid(True, alpha=0.3, axis='x', linestyle='--')
ax2.set_xlim(0, 5)
plt.tight_layout()
plt.savefig(os.path.join(output_folder, "figure1_panelB_MeanRanks.pdf"), bbox_inches='tight')
plt.savefig(os.path.join(output_folder, "figure1_panelB_MeanRanks.png"), dpi=300, bbox_inches='tight')
plt.close(fig_b)

# ============================================================================
# PANEL C: Effect Sizes
# ============================================================================
fig_c, ax3 = plt.subplots(figsize=(11, 9))
baselines = ['MIDA', 'GAIN', 'DAE', 'MICE']
colors_effect = ['#27ae60' if e < -0.5 else '#2ecc71' if e < -0.3 else '#95a5a6' if e < 0
                 else '#ecf0f1' if e < 0.3 else '#e67e22' if e < 0.5 else '#e74c3c'
                 for e in effects]

bars = ax3.barh(baselines, effects, color=colors_effect,
                alpha=0.85, edgecolor='black', linewidth=1.2)

for i, (baseline, r) in enumerate(zip(baselines, effects)):
    x_pos = r + 0.05 if r > 0 else r - 0.05
    ha = 'left' if r > 0 else 'right'
    ax3.text(x_pos, i, f'{r:+.2f}', va='center', ha=ha, fontweight='bold')

ax3.axvline(x=0, color='black', linestyle='-', linewidth=2)
ax3.axvline(x=-0.3, color='green', linestyle=':', alpha=0.6, linewidth=1.5)
ax3.axvline(x=-0.5, color='green', linestyle='--', alpha=0.6, linewidth=1.5)
ax3.axvline(x=0.3, color='red', linestyle=':', alpha=0.6, linewidth=1.5)

ax3.tick_params(axis='x', labelsize=14)
ax3.tick_params(axis='y', labelsize=14)
ax3.set_xlabel('Rank-biserial correlation (r)', fontweight='bold')
ax3.set_title('Effect sizes: our technique vs. baselines', fontweight='bold') #, loc='left')
ax3.grid(True, alpha=0.3, axis='x', linestyle='--')
ax3.set_xlim(-1.2, 0.7)
ax3.text(0.78, 0.98, '− = Our Technique Better\n+ = Baseline Better',
         transform=ax3.transAxes, verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5)) #, fontsize=9
plt.tight_layout()
plt.savefig(os.path.join(output_folder, "figure1_panelC_EffectSizes.pdf"), bbox_inches='tight')
plt.savefig(os.path.join(output_folder, "figure1_panelC_EffectSizes.png"), dpi=300, bbox_inches='tight')
plt.close(fig_c)

# ============================================================================
# PANEL D: Boxplots
# ============================================================================
fig_d, ax4 = plt.subplots(figsize=(10, 10))
data_box = [methods_dict[m] for m in methods_order]
bp = ax4.boxplot(data_box, positions=range(len(methods_order)), patch_artist=True,
                 widths=0.6, showmeans=True, meanline=True,
                 boxprops=dict(linewidth=1.5, edgecolor='black'),
                 whiskerprops=dict(linewidth=1.5),
                 capprops=dict(linewidth=1.5),
                 medianprops=dict(linewidth=2, color='red'),
                 meanprops=dict(linewidth=2, color='blue', linestyle='--'))

for i, (box, method) in enumerate(zip(bp['boxes'], methods_order)):
    box.set_facecolor(colors[method])
    box.set_alpha(0.7)

for i, (method, values) in enumerate(zip(methods_order, data_box)):
    x = np.random.normal(i, 0.04, size=len(values))
    ax4.scatter(x, values, alpha=0.6, s=50, color='black', zorder=10)

ax4.set_xticks(range(len(methods_order)))
ax4.set_xticklabels(methods_order, rotation=30, ha='right', fontweight='bold')
ax4.tick_params(axis='x', labelsize=14)
ax4.tick_params(axis='y', labelsize=14)
ax4.set_ylabel('RMSE', fontweight='bold')
ax4.set_title('RMSE distribution by method', fontweight='bold') #, loc='left')
ax4.grid(True, alpha=0.3, axis='y', linestyle='--')
ax4.plot([], [], 'r-', linewidth=2, label='Median')
ax4.plot([], [], 'b--', linewidth=2, label='Mean')
ax4.legend(loc='upper left')#, fontsize=9)
plt.tight_layout()
plt.savefig(os.path.join(output_folder, "figure1_panelD_Boxplots.pdf"), bbox_inches='tight')
plt.savefig(os.path.join(output_folder, "figure1_panelD_Boxplots.png"), dpi=300, bbox_inches='tight')
plt.close(fig_d)


print(f"✓ Saved individual PDFs to '{output_folder}':")
print(f"  - {os.path.join(output_folder, 'figure1_panelA_RMSE_Trajectories.pdf')}")
print(f"  - {os.path.join(output_folder, 'figure1_panelB_MeanRanks.pdf')}")
print(f"  - {os.path.join(output_folder, 'figure1_panelC_EffectSizes.pdf')}")
print(f"  - {os.path.join(output_folder, 'figure1_panelD_Boxplots.pdf')}")

# =============================================================================
# SPEARMAN CORRELATION ANALYSIS (ρ)
# =============================================================================
print("\n\n SPEARMAN CORRELATION ANALYSIS (ρ)")
print("-" * 80)

# Example: Batch Evidence vs Performance Gain (%)
# -----------------------------------------------
# Assign numeric batch evidence levels (e.g., 1=Low, 2=Medium, 3=High)
batch_evidence = np.array([2, 3, 1, 1, 2])   # [Boston, Glass, Sonar, Breast, Credit]
performance_gain = np.array([-6.5, 22.8, -7.5, 0.8, 7.9])  # % difference vs best baseline

# Compute Spearman correlation (non-parametric rank-based)
rho, p_rho = spearmanr(batch_evidence, performance_gain)

print(f"Spearman’s ρ (rho): {rho:.3f}")
print(f"p-value: {p_rho:.4f}")

# Interpret results
if p_rho < 0.001:
    sig_label = "*** (Highly significant)"
elif p_rho < 0.01:
    sig_label = "** (Significant)"
elif p_rho < 0.05:
    sig_label = "* (Marginally significant)"
else:
    sig_label = "n.s. (Not significant)"

# Simple visual check
plt.figure(figsize=(8, 6))
sns.regplot(x=batch_evidence, y=performance_gain, ci=None, scatter_kws={'s': 80})
plt.title(f"Spearman Correlation: ρ = {rho:.2f} ({sig_label})", fontweight="bold")
plt.xlabel("Batch Evidence (1=Low, 2=Medium, 3=High)", fontweight="bold")
plt.ylabel("Performance Gain (%)", fontweight="bold")
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig("spearman_rho_analysis.png", dpi=300, bbox_inches="tight")

print(f"Saved correlation plot to 'spearman_rho_analysis.png'")
print(f"Interpretation: A positive ρ indicates stronger batch evidence correlates "
      f"with larger performance improvements.")